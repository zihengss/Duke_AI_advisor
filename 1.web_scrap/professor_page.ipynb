{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_page to all professor's personal page\n",
    "web_page = 'https://cs.duke.edu/people/appointed-faculty/primary-faculty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal URLs found:\n",
      "['https://scholars.duke.edu/person/debmalya.panigrahi', 'https://scholars.duke.edu/person/brandon.fain', 'https://scholars.duke.edu/person/tomasi', 'https://scholars.duke.edu/person/alberto', 'https://scholars.duke.edu/person/ashwin', 'https://scholars.duke.edu/person/Kristin.Stephens-Martinez', 'https://scholars.duke.edu/person/rongge', 'https://scholars.duke.edu/person/tananun.songdechakraiwut', 'https://scholars.duke.edu/person/sudeepa', 'https://scholars.duke.edu/person/xiaowei.yang', 'https://scholars.duke.edu/person/kamesh', 'https://scholars.duke.edu/person/nicki.washington', 'https://scholars.duke.edu/person/rcd', 'https://scholars.duke.edu/person/monica.agrawal', 'https://scholars.duke.edu/person/rudin', 'https://scholars.duke.edu/person/raluca.gordan', 'https://scholars.duke.edu/person/yesenia.velasco', 'https://scholars.duke.edu/person/Jian.Pei', 'https://scholars.duke.edu/person/rodger', 'https://scholars.duke.edu/person/pardis.emami_naeini', 'https://scholars.duke.edu/person/danyang.zhuo', 'https://scholars.duke.edu/person/Danfeng.Zhang', 'https://scholars.duke.edu/person/bhuwan.dhingra', 'https://scholars.duke.edu/person/bruce.donald', 'https://scholars.duke.edu/person/anru.zhang', 'https://scholars.duke.edu/person/bmm', 'https://scholars.duke.edu/person/reif', 'https://scholars.duke.edu/person/Benjamin.Rossman', 'https://scholars.duke.edu/person/Matthew.Lentz', 'https://scholars.duke.edu/person/robert.calderbank', 'https://scholars.duke.edu/person/pankaj', 'https://scholars.duke.edu/person/Kartik.Nayak', 'https://scholars.duke.edu/person/junyang', 'https://scholars.duke.edu/person/eric.fouhmbindi', 'https://scholars.duke.edu/person/xiaobai.sun', 'https://scholars.duke.edu/person/Michael.Reiter', 'https://scholars.duke.edu/person/kate.o.hanlon', 'https://scholars.duke.edu/person/Lisa.Wills', 'https://scholars.duke.edu/person/amink', 'https://scholars.duke.edu/person/alexander.steiger', 'https://scholars.duke.edu/person/parr', 'https://scholars.duke.edu/person/ola', 'https://scholars.duke.edu/person/chase']\n",
      "in total, we have 43 unique pages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://cs.duke.edu/people/appointed-faculty/primary-faculty\"\n",
    "all_personal_pages = set()\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all links (anchor tags) in the webpage\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter and print URLs that could be personal pages\n",
    "    print(\"Personal URLs found:\")\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        # Check if the link contains \"http\" (external link) and possibly represents a personal page\n",
    "        if \"https://scholars.duke.edu/person/\" in href:\n",
    "            all_personal_pages.add(href)\n",
    "    all_personal_pages = list(all_personal_pages)\n",
    "    print(all_personal_pages)\n",
    "    print(f'in total, we have {len(all_personal_pages)} unique pages')\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overview(url, cnt=0):\n",
    "    rst = ''\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the div with class 'excerpt'\n",
    "        excerpt_div = soup.find('div', class_='excerpt')\n",
    "        name_span = soup.find('span', class_='pr-3 leading-d1') \n",
    "\n",
    "        \n",
    "        # Extract the data-excerpt-text or the full-text if available\n",
    "        if excerpt_div and name_span:\n",
    "            cleaned_text = excerpt_div.get_text(strip=True)  # Strips leading/trailing whitespace\n",
    "            name_span = name_span.get_text(strip=True)\n",
    "            rst = f'Overview of professor {name_span}: \\n' + cleaned_text\n",
    "            return rst\n",
    "        else:\n",
    "            rst = \"No Overview\"\n",
    "            return rst\n",
    "    else:\n",
    "        if cnt>2:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "            print(url)\n",
    "            print('-----------')\n",
    "        else:\n",
    "            get_overview(url, cnt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of professor Pankaj K. Agarwal: \n",
      "Geometric algorithms, discrete geometry, geometric data analysis, data structures, database systems and data mining, robotics algorithms, geographic information systems.\n"
     ]
    }
   ],
   "source": [
    "print(get_overview('https://scholars.duke.edu/person/pankaj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_string_to_file(text, filename):\n",
    "    \"\"\"\n",
    "    Save a string to a text file, creating the folder if it doesn't exist.\n",
    "    \n",
    "    :param text: The string to save\n",
    "    :param filename: The name of the file, including its path\n",
    "    \"\"\"\n",
    "    # Extract the directory from the filename\n",
    "    folder = os.path.dirname(filename)\n",
    "    \n",
    "    # Create the folder if it doesn't exist\n",
    "    if folder and not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # Write the text to the file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 43/43 [00:16<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for url in tqdm(all_personal_pages, desc=\"Processing URLs\"):\n",
    "    text = get_overview(url)\n",
    "    save_string_to_file(text, '../data/rag_data/professor/'+url.split('/')[-1]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/amink.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/anru.zhang.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/bhuwan.dhingra.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/bruce.donald.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Jian.Pei.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/danyang.zhuo.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Michael.Reiter.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/ola.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/pardis.emami_naeini.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/alexander.steiger.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/xiaowei.yang.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/pankaj.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/chase.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/raluca.gordan.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/reif.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/brandon.fain.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/robert.calderbank.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/tananun.songdechakraiwut.txt (contains 'No Overview')\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Lisa.Wills.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/rudin.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/yesenia.velasco.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/rongge.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/bmm.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Kristin.Stephens-Martinez.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/tomasi.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/kate.o.hanlon.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/parr.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Kartik.Nayak.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Danfeng.Zhang.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/debmalya.panigrahi.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/xiaobai.sun.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Matthew.Lentz.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/rodger.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/ashwin.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/Benjamin.Rossman.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/rcd.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/junyang.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/nicki.washington.txt (contains 'No Overview')\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/monica.agrawal.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/alberto.txt\n",
      "Deleted file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/eric.fouhmbindi.txt (contains 'No Overview')\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/kamesh.txt\n",
      "Processed file: /Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor/sudeepa.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory = '/Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor'\n",
    "\n",
    "# Iterate over all .txt files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the file and process the lines\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Remove empty lines and strip leading whitespace\n",
    "        cleaned_lines = [line.lstrip() for line in lines if line.strip()]\n",
    "\n",
    "        # Check if the file contains \"No Overview\" and delete it if so\n",
    "        if any(\"No Overview\" in line for line in cleaned_lines):\n",
    "            os.remove(filepath)\n",
    "            print(f\"Deleted file: {filepath} (contains 'No Overview')\")\n",
    "        else:\n",
    "            # Write the cleaned lines back to the file\n",
    "            with open(filepath, 'w') as file:\n",
    "                file.writelines(cleaned_lines)\n",
    "            print(f\"Processed file: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now the folder have total 33 files\n"
     ]
    }
   ],
   "source": [
    "print(f'now the folder have total {len(os.listdir(directory))} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amink.txt has 31 words\n",
      "bhuwan.dhingra.txt has 18 words\n",
      "bruce.donald.txt has 38 words\n",
      "Jian.Pei.txt has 19 words\n",
      "danyang.zhuo.txt has 21 words\n",
      "Michael.Reiter.txt has 74 words\n",
      "ola.txt has 18 words\n",
      "pardis.emami_naeini.txt has 93 words\n",
      "xiaowei.yang.txt has 9 words\n",
      "pankaj.txt has 25 words\n",
      "chase.txt has 21 words\n",
      "reif.txt has 28 words\n",
      "brandon.fain.txt has 30 words\n",
      "robert.calderbank.txt has 413 words\n",
      "rudin.txt has 312 words\n",
      "yesenia.velasco.txt has 19 words\n",
      "rongge.txt has 11 words\n",
      "bmm.txt has 24 words\n",
      "Kristin.Stephens-Martinez.txt has 120 words\n",
      "tomasi.txt has 58 words\n",
      "kate.o.hanlon.txt has 19 words\n",
      "parr.txt has 24 words\n",
      "Kartik.Nayak.txt has 30 words\n",
      "debmalya.panigrahi.txt has 22 words\n",
      "xiaobai.sun.txt has 32 words\n",
      "Matthew.Lentz.txt has 12 words\n",
      "rodger.txt has 11 words\n",
      "Benjamin.Rossman.txt has 58 words\n",
      "rcd.txt has 19 words\n",
      "junyang.txt has 60 words\n",
      "alberto.txt has 109 words\n",
      "kamesh.txt has 43 words\n",
      "sudeepa.txt has 204 words\n",
      "Average word count per file: 61.36\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory = '/Users/zihengs/Desktop/Duke_AI_advisor/data/rag_data/professor'\n",
    "\n",
    "# Initialize counters for total word count and file count\n",
    "total_word_count = 0\n",
    "file_count = 0\n",
    "\n",
    "# Iterate over all .txt files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the file and count the words\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            word_count = sum(len(line.split()) for line in lines)  # Count words in each line\n",
    "        \n",
    "        # Update counters\n",
    "        total_word_count += word_count\n",
    "        file_count += 1\n",
    "        \n",
    "        print(f\"{filename} has {word_count} words\")\n",
    "\n",
    "# Calculate and print the average word count\n",
    "if file_count > 0:\n",
    "    average_word_count = total_word_count / file_count\n",
    "    print(f\"Average word count per file: {average_word_count:.2f}\")\n",
    "else:\n",
    "    print(\"No .txt files found in the directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
